{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis notebook covers a simple exploratory analysis over the dataset (dataset name), in addition, we shortlisted a few machine learning models to predict the slowness in traffic variable (0-100%). Although a very interesting dataset, its size is small (135 instances), limiting the models performance and the exploratory analysis. **This notebook was built with self-learning purposes only, we'll be happy to have your feedback on misunderstandings and improvements.** The dataset covers 135 instances, each representing 30 minute timeframes over the week, from Monday 14 to Friday 18.\n\nA special thanks to Aurelien Geron, author of *Hands-On Machine Learning* (*O'Reilly*) for the book lessons that guided us through this notebook. Similary we thank the original owner and donors of the dataset [*Behavior of the urban traffic of the city of Sao Paulo in Brazil Data Set*](https://archive.ics.uci.edu/ml/datasets/Behavior+of+the+urban+traffic+of+the+city+of+Sao+Paulo+in+Brazil),  used in this notebook: Ricardo Pinto Ferreira, Andrea Martiniano and Renato Jose Sassi.\n\nThis notebook was made by Guilherme Matunaga and Paris Mollo.\n\n* [1. Data Overview](#a)\n* [2. Data Manipulation](#b)\n* [3. Data Visualization](#c)\n* [4. Data preprocessing](#d)\n* [5. Model Building](#e)\n* [6. Model Performance](#f)\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\n\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Hour column in the dataset is coded, the \"dictionary\" below translates the codes into the actual time during the day. Similary, we created that same logic for the weekdays.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"HOURS = {7:10, 7:30, 8:00, 8:30, 9:00, 9:30, 10:00, 10:30, 11:00, 11:30, 12:00, 12:30, 13:00, 13:30, 14:00, 14:30, 15:00, 15:30, 16:00, 16:30, 17:00, 17:30, 18:00, 18:30, 19:00, 19:30, 20:00} # reference from dataset folder\nDATA_PATH = \"../input/sp-urban-traffic/urban_traffic_sp.csv\"\nTARGET_VAR = \"Slowness in traffic (%)\"\nMONDAY  = 26\nTUESDAY = 53\nWEDNESDAY = 80\nTHURSDAY = 107\nFRIDAY = 134\nDAYS_TO_CODE = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"a\">1. Data Overview</div>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(path:str=DATA_PATH, sep:str=\";\"):\n    try:\n        df = pd.read_csv(path, sep=sep)\n        print(\"Data loaded with success\")\n        return df\n    except FileNotFoundError:\n        print(\"Check your data directory! Nothing there yet...\")\n        return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = load_data()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"luckily there are no null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Dataframe shape: {df.shape}\\n\")\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(bins=50, figsize=(20, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"b\">2. Data Manipulation</div>\nLet's convert the slowness in traffic variable into a float object for statistical purposes!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_target(df, target_var=TARGET_VAR, to=float):\n    df[target_var] = df[target_var].str.replace(',', '.').astype(to)\ntransform_target(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we make available a function so that you can create the day column as weekday nomenclature or as ordinal numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# New attribute\ndef transform_days(df, create_column=False, to_numerical=False):\n    #check is day column exists if not create\n    #if numerical transformation, go from day to number\n    #else go from number to day names\n    if create_column:\n        df['Day'] = '0'\n\n    position=-1\n    if to_numerical is False:\n        for idx in df.index:\n            if idx <= MONDAY:\n                df.iloc[idx, position] = 'Monday'\n            elif idx <= TUESDAY:\n                df.iloc[idx, position] = 'Tuesday'\n            elif idx <= WEDNESDAY:\n                df.iloc[idx, position] = 'Wednesday'\n            elif idx <= THURSDAY:\n                df.iloc[idx, position] = 'Thursday'\n            elif idx <= FRIDAY:\n                df.iloc[idx, position] = 'Friday'\n    else:\n        df_values = df[\"Day\"].unique()\n        for key, value in DAYS_TO_CODE.items():\n            assert key in df_values, \"First transform your data into weekday by setting to_numerical=False, then apply the numerical transformation\"\n            df.loc[(df.Day == key), 'Day'] = value\n        df['Day'] = df['Day'].astype(int)\n        \ntransform_days(df, create_column=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset instances represent each a 30 minute period that starts at 7:00 AM until 8:00 PM, here is the function that deals with that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create code to hour dict\ndef set_hours_dict(df, hours:dict =HOURS)-> dict:\n    hours_arr = []\n\n    for hour, minute in hours.items():\n      s1 = str(hour) + ':' + '00'\n      s2 = str(hour) + ':' + str(minute)\n      if hour != 20:\n        hours_arr.append(s1)\n        hours_arr.append(s2)\n      else:\n        hours_arr.append(s1)\n\n    code_to_hour = {}\n    for code, hour in zip(df['Hour (Coded)'], hours_arr):\n      code_to_hour[code] = hour\n\n    return code_to_hour\n\ncode_to_hour = set_hours_dict(df)\n\ndef code_hour(code):\n  return code_to_hour[code]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"c\">3. Exploratory Data Analysis</div>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Correlation matrices","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_days(df, to_numerical=True)\nplt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), cmap='Blues', annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df.corr()\ncorr_matrix['Slowness in traffic (%)'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the time of the day is the variable that has the largest impact on the slowness in traffic among the other features. Features such as lack of electricity and flooding have a considerable effect on the target variable as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = [\"Slowness in traffic (%)\", \"Hour (Coded)\"]\nscatter_matrix(df[attributes], figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot overtime","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def slowness_over_time(df, coded_hours=False):\n    fig = plt.figure(figsize=(20, 12))\n    ax = fig.add_axes([0, 0, 1, 1])\n\n    colors = {'Monday': 'r', 'Tuesday': 'b', 'Wednesday': 'g', 'Thursday': 'yellow', 'Friday':'black'}\n    transform_days(df)\n    for e in df['Day'].unique():\n        subset = df[df['Day'] == e]\n        ax.plot(subset['Hour (Coded)'], subset['Slowness in traffic (%)'],color=colors[e])\n\n    ax.set_title('Slowness in traffic VS. Hour of the day', fontsize=25, pad=15)\n    ax.set_xlabel('Hour of the day', fontsize=15)\n    ax.set_ylabel('Slowness in traffic (%)', fontsize=15)\n    \n    if coded_hours is False:\n        ax.set_xticks(range(1, 28))\n        ax.set_xticklabels(map(code_hour, subset['Hour (Coded)'].unique()))\n\n    ax.legend(colors, fontsize=20)\n\n    plt.show()\nslowness_over_time(df)\ntransform_days(df, to_numerical=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Traffic peaks in the late afternoon and early evening exceed those in the morning. Wednesday has the highest peak in traffic while Monday afternoon is the period with the lowest traffic levels.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Boxplot","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We decided not to remove the outliers, since the dataset is small and outliers in this situation represents an important characteristc of traffic peaks, the \"rush hour\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = df.nunique()[df.nunique() > 2].keys() # Look at non-categorical data\nnum_cols = num_cols.drop('Day')\n\nl = num_cols.values\nnumber_of_columns=len(num_cols.values)\nnumber_of_rows = len(l)-1/number_of_columns\nplt.figure(figsize=(number_of_columns,5*number_of_rows))\n\nfor i in range(0,len(l)):\n    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n    sns.set_style('whitegrid')\n    sns.boxplot(df[l[i]],color='green',orient='v')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('Day')['Slowness in traffic (%)'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"d\">4. Data Preprocessing</div>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Defining sets\nHere we create the test and train set, we decided to maintain the frequency of each 30 minute period among both sets for generalization purposes (avoiding a bias model). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"Hour (Coded)\"])\n\n# Verifying stratified distribution\nprint(\"Train set class proportions:\\n\")\nprint(train_set[\"Hour (Coded)\"].value_counts() / len(train_set))\nprint(\"\\nFull set:\")\nprint(df[\"Hour (Coded)\"].value_counts() / len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_set.drop(\"Slowness in traffic (%)\", axis=1)\ny_train = train_set[\"Slowness in traffic (%)\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we're preparing a numerical set in order to perform scaling before fitting the model, by removing the categorical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_num_cols = X_train.nunique()[X_train.nunique() > 2].keys()\nx_num_cols = x_num_cols.drop('Day')\nnumerical_data = list(x_num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transformation Pipelines","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We apply the standard scaling on the numerical features through the scikit-learn classes Pipeline and Column Transformer. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pipeline = Pipeline([('std_scaler', StandardScaler())])\nfull_pipeline = ColumnTransformer([(\"num\", num_pipeline, numerical_data)], remainder='passthrough')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_prepared = full_pipeline.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"e\">5. Model Building</div>\nWe selected a few machine learning models based on the book (*Hands On Machine Learning*) notebook and trained them using 10 fold cross-validation. We then preselected the model with the best performance (lowest score) and ran a GridSearch in order to determine the best hyperparameters for optimization purposes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_reg = DecisionTreeRegressor(random_state=42)\nscores = cross_val_score(tree_reg, X_train_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_reg = SVR(kernel=\"linear\")\nsvr_scores = cross_val_score(svm_reg, X_train_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nsvr_rmse_scores = np.sqrt(-svr_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\nlin_scores = cross_val_score(lin_reg, X_train_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_scores = cross_val_score(forest_reg, X_train_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"f\">6. Model Performances</div>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {\n    \"Model\":[\"Linear Reg\", \"Decision Tree\", \"SVR\", \"Random Forest\"],\n    \"Mean Score\": [lin_rmse_scores.mean(), tree_rmse_scores.mean(), svr_rmse_scores.mean(), forest_rmse_scores.mean()],\n    \"Standard Deviation\": [lin_rmse_scores.std(), tree_rmse_scores.std(), svr_rmse_scores.std(), forest_rmse_scores.std()]\n}\nscores_df = pd.DataFrame(data)\nscores_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8, 10, 12, 14, 16, 18]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [10, 12, 14, 16, 18]},\n]\n\nforest_reg = RandomForestRegressor(random_state=42)\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(X_train_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_grid_model = grid_search.best_estimator_\nprint(\"Best model paramateres:\", grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can find what the model defines as the most important features in predicting the slowness in traffic after learning with the train set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\nsorted(zip(feature_importances, list(X_train)), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\n\nX_test = test_set.drop(\"Slowness in traffic (%)\", axis=1)\ny_test = test_set[\"Slowness in traffic (%)\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are a few examples...We are not (yet) urban traffic experts, so defining a acceptable error margin isn't clear for us. We would love to have your feedback on that!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = final_predictions[:10]\nactual_results = y_test[:10]\n\nfor p, a in zip(predictions, actual_results):\n    print(\"Predicted: {:.2f} - Expected: {}\".format(p, a))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}